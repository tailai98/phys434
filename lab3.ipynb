{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[10., 10.3, 2.1, 0., 0., 15.6, 22.3, 12.7]\n",
    "data=stats.norm.rvs(loc = 12., scale = 0.4, size = 100000)\n",
    "full = np.append(data, a)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(d,100, density=True)\n",
    "plt.xlabel(\"temperature reading\")\n",
    "plt.ylabel(\"Probability in log\")\n",
    "plt.yscale('log')\n",
    "#x = np.linspace(4.95,5.05,1000)\n",
    "#ax.plot(x,stats.norm.pdf(x,loc = 5., scale = 0.01),linewidth = 8,alpha = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2:QUESTION : What data point are within 5σ of the background distribution? The reason I choose this question is because this is the real data we keep that doesn't contain any anomalous read.\n",
    "\n",
    "A3:Calculate the data inside the 5σ\n",
    "\n",
    "A4:Truth table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.sort(full)\n",
    "D = np.array([])\n",
    "D_1 = np.array([])\n",
    "\n",
    "def truthtable(s):\n",
    "    sigma = stats.norm.sf(s)\n",
    "    maxTemp = stats.norm.isf(sigma, loc=12, scale=0.4)\n",
    "    minTemp = stats.norm.isf(1-sigma, loc=12, scale=0.4)\n",
    "    D = X[np.logical_and(X > minTemp, X < maxTemp)]\n",
    "    D_1 = X[np.logical_or(X < minTemp, X > maxTemp)]\n",
    "    TP = sum((Counter(data) & Counter(D)).values())\n",
    "    FP = sum((Counter(D) & Counter(a)).values())\n",
    "    TN = sum((Counter(data) & Counter(D_1)).values())\n",
    "    FN = sum((Counter(D_1) & Counter(a)).values())\n",
    "    \n",
    "    table = pd.DataFrame(data=[[TP,FP],[TN,FN]],\n",
    "                     index=[\"Pos\", \"Neg\"],\n",
    "                     columns=[\"True\", \"False\"])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truthtable(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two mistakes(12.7 and 10.3) , if we included all data at greater than 5 sigma significance there would be more FP less TN less FN\n",
    "\n",
    "B:Counstruct truth table for different threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truthtable(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When we choose 3, TP decresed by one and TN increase by one, however we need to thrown out 264 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truthtable(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we choose 7, TP increased by one and TN decreased by one(increase errors of commission and reduce the errors of omission)so we can have more real data.\n",
    "\n",
    "C:contruct a true table for all thrown out 12.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = stats.norm.isf(stats.norm.sf(12.7, scale=0.4, loc=12))\n",
    "truthtable(sigma)\n",
    "sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We need to thrown out 8019 of real data in order to remove all mistakes of commission and about 1.75 sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\"What recorded temperature value on the thermal device is five sigmas or more from the mean so that we can get rid of these points of data? In more detail, at which x value of temperature in the new distribution (with 'bad data') is the probability of getting the temperature larger than the probability of five sigma to the right of the mean or less than the probability of five sigma to the left of the mean in a normal distribution?\" We do this by finding the integral under the distribution we analyze at x that yields the same or less probability than 5 sigma on a normal distribution. We could search for five sigma of our new dataset by searching all values larger or less than the mean +- stdev * 5 but this doesn't neccessarily give us the probability of five sigma in a normal distribution which is what we're looking for. This method only works for a normal distribution and for any other distribution it would not work. This is my question because I am essentially asking \"Which data points have a probability larger or less than 5 sigma?\" in order to determine which of the data points are \"bad data\" and thus worthy of removing. We are using 5 sigma as a base line because the probability of 5 sigma or higher is about 1 in 3.5 million which is reasonable cause to see the data point as an outlier. We also use this baseline because if we go any lower we may be recieving too many false positives and thus losing out on good data. There is discussion to be had whether its better to have more false positives but less true negative or vice versa but we decided it is better to have less false positives and more true negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiveSigma = 1 - stats.norm.cdf(5)\n",
    "xTop = stats.norm.ppf(1 - fiveSigma, mu, std)\n",
    "xBot = stats.norm.ppf(fiveSigma, mu, std)\n",
    "print(xTop)\n",
    "print(xBot)\n",
    "\n",
    "for i in reversed(range(len(d))):\n",
    "    if d[i] > xTop:\n",
    "        d = np.delete(d, i)\n",
    "    if d[i] < xBot:\n",
    "        d = np.delete(d, i)\n",
    "print(len(d))\n",
    "plt.hist(d, 100, density=True)\n",
    "plt.yscale('log')\n",
    "plt.title('Distribution of cleaned data')\n",
    "plt.xlabel('Temperature (k)')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()\n",
    "x = list(range(len(d)))\n",
    "plt.title('Plot of cleaned data points with respect to their index values')\n",
    "plt.xlabel('Index value')\n",
    "plt.ylabel('Value')\n",
    "plt.plot(x, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First we calculated the 5 sigma probability of a normal distribution and then we calculated the ppf, of a normal distribution with the adjusted mean of 12 and standard deviation of .4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical threshold determines the number of sigmas away from the mean. This affects the number of omissions greatly and deciding this cutoff is a game of deciding what number of false positives vs true negatives you are comfortable with. For example, if you want to make sure you are keeping all the good dat' you might want to have a larger sigma cut off so you are including more values, thus making sure to keep the good values. However, this might come at the cost of creating more false positives because the sensitivity to deviation is weaker and so more bad data may be left in the distribution. On the other hand, if we want to make sure you are getting rid of all of the bad data you would decrease the sigma cutoff so that you can make sure more of your bad data on the edges of the distribution are caught and removed. However, this comes at the cost of removing more good data as the sensitivity to deviation is much higher and some good data will be mistaken for bad data. Thus a general rule is that the higher the sigma, the less false positives but more true negatives and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sure that there arr some bad datas getting in. How muc they affect the result is depends on the threshold of our data system. They are unavoidable, since it can by lower by lowering (making more sensitive) our statistical threshold but this comes at the cost of potentially more omissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.vstack((stats.norm.rvs( scale = 1, size = 100000), stats.norm.rvs( scale = 1, size = 100000)))\n",
    "a.shape\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "h = ax.hist2d(a[0,:],a[1,:],bins=100, density=True);\n",
    "ax.set_aspect('equal', 'box')\n",
    "plt.xlim([-3 , 3])\n",
    "plt.ylim([-3 , 3])\n",
    "plt.title(\"2D Histogram of positional uncertainty\", fontsize = 24)\n",
    "plt.ylabel(\"$\\Delta$y arcseconds\", fontsize = 18)\n",
    "plt.xlabel(\"$\\Delta$x arcseconds\", fontsize = 18)\n",
    "plt.colorbar(h[3], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We wounder the distance in arcseconds of a '5 sigma' detection of movement. Thus we are looking for the distance a potential asteroid must travel to be considered an asteroid. Thus, \"What distance away from the origin would confirm a potential asteroid as an asteroid. Or more mathematically, in a 2D Gaussian with 1 arcsecond RMS with a significance sensitivity of 5 sigma, at what distance foes a potential asteroid reach 5 sigma away from average and is thus not considered an asteroid?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the distance away from the origin at which the 2-dimensional Gaussian background might produce a position measurement that has a significance of $5\\sigma$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_2d = stats.rayleigh(scale=1)\n",
    "distance = dist_2d.ppf(dist.cdf(5))\n",
    "pdf_2d = lambda r: r*np.exp(-r**2 / 2)\n",
    "value, error = sp.integrate.quad(pdf_2d, distance, np.inf)\n",
    "print(f\"Distance for 5 sigma: {distance} arc-seconds\")\n",
    "print(f\"Converting back to sigma: {dist.ppf(1 - value)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that safe to say that a moon is blocking cosmic rays from reaching earth if we have observed 6800 cosmic rays over 15 days? What is the probability of observing 6800 cosmic rays or within 15 days? Will the calculation based on the distribution of expected cosmic rays observed of 1 cosmic ray per minute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the probability if we set mu = 1 (cosmic ray / minute) (60 minutes / hour) 8(hour / day) * 15 (days) = 7200, and k = P(k <= 6800) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6800\n",
    "mu = 7200\n",
    "prob = stats.poisson.cdf(k, mu)\n",
    "print(\"Probability of 6800 rays or less: \" + str(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = stats.norm.ppf(prob)\n",
    "print(\"Sigma of 6800: \" + str(sig))\n",
    "print(\"Magnitude of sigma: \" + str(abs(sig)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the sigma value is less than 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
